{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TJfNUgelLKKI"
   },
   "source": [
    "# <center>Machine learning from scratch - Part II</center>\n",
    "## <center>WebValley ReImagined 2021</center>\n",
    "### <center>Marco Chierici</center>\n",
    "#### <center>FBK/DSH</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fuVtNnbq1o3z"
   },
   "source": [
    "In this handout we will go through basic concepts of machine learning using Python and scikit-learn on a real-world dataset of biological relevance from [The Cancer Genome Atlas](https://www.cancer.gov/about-nci/organization/ccg/research/structural-genomics/tcga) (TCGA) program."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Breast cancer dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "n012d52Z1x2_"
   },
   "source": [
    "The data include gene expression of **499 patients** (already split into 399/100 training/test sets) with **breast invasive carcinoma (BRCA)**, aiming at predicting the **estrogen receptor status** (positive vs negative samples).\n",
    "\n",
    "The data was preprocessed a little bit to facilitate the progress of the tutorial.\n",
    "\n",
    "Let's start by loading a few modules that we'll be using later:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ww5yKJJpk3Wc"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pylab as plt\n",
    "import pandas as pd\n",
    "from sklearn import neighbors\n",
    "from pathlib import Path ## for creating paths in a neat way"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "X1JAqMhok3Wf"
   },
   "source": [
    "Define files to read:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VjZyyr-EoCQw"
   },
   "outputs": [],
   "source": [
    "DATA_DIR = Path(\"data\")\n",
    "DATA_TR = DATA_DIR / \"brca_genes_tr.tsv.gz\"\n",
    "DATA_TS = DATA_DIR / \"brca_genes_ts.tsv.gz\"\n",
    "DATA_TS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Note:_ from now on we will use the \"tr\" suffix to denote the training set, and \"ts\" for the test set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QywnF5Rek3Wj"
   },
   "source": [
    "Read the files in as _pandas dataframes_:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 864
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1345,
     "status": "error",
     "timestamp": 1554388310330,
     "user": {
      "displayName": "Marco Chierici",
      "photoUrl": "https://lh6.googleusercontent.com/-1LjDBMGAnW8/AAAAAAAAAAI/AAAAAAAACB0/ScmrJqjZC-4/s64/photo.jpg",
      "userId": "06871654247545486268"
     },
     "user_tz": -120
    },
    "id": "cYnTRFVyk3Wk",
    "outputId": "a91effd6-a06f-42b9-8911-0ee006aaa08a"
   },
   "outputs": [],
   "source": [
    "data_tr = pd.read_csv(DATA_TR, sep=\"\\t\")\n",
    "data_ts = pd.read_csv(DATA_TS, sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "THEk0UfNqZqZ"
   },
   "source": [
    "The function `read_csv` has a lot more input arguments to deal with different situations.\n",
    "\n",
    "If you want to know more about this or any other Python function, use the `help(function_name)` command or, within a notebook, `function_name?`.\n",
    "\n",
    "What do we have here? Start with getting the dimensions of what we just loaded:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2iltS1Q-k3Wn",
    "outputId": "ea81462d-8a49-406b-c933-182c49379053"
   },
   "outputs": [],
   "source": [
    "data_tr.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_kRnmCcI2Wo4"
   },
   "source": [
    "What's inside?\n",
    "\n",
    "A peek at the first rows reveals that the first column (the dataframe index) contains the sample IDs, then we have three more columns with what seems clinical data, and the remaining columns are genes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TVfPwU6-k3Wt",
    "outputId": "884dd460-6c53-4bf4-9c37-f7c7299b37a2"
   },
   "outputs": [],
   "source": [
    "data_tr.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-Q-p18V3k3Ww"
   },
   "source": [
    "Note the use of **prefixing** in column names (`gene`): not only does it add a level of information on the content of the variables, but it is useful since it allows selecting or filtering out groups of variables in a simpler way.\n",
    "\n",
    "Drop the first column from the train and test expression sets, since it's just the sample IDs (we put them in to be able to check whether samples and labels match, but once we are sure of what we are doing we don't really need them anymore)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "F1bUOeE4k3Wx"
   },
   "outputs": [],
   "source": [
    "data_tr = data_tr.drop('Sample', axis=1)\n",
    "data_ts = data_ts.drop('Sample', axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WA_GLQAWk3W0"
   },
   "source": [
    "Check what happened"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QgcQgBVek3W1",
    "outputId": "6cbec2e0-0001-4e03-c040-0bbddd51db5b"
   },
   "outputs": [],
   "source": [
    "data_tr.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Bw2huNKIk3W8"
   },
   "source": [
    "`stage` is the tumor stage according to the American Joint Committee on Cancer (AJCC).\n",
    "\n",
    "`ER` is the status of the Estrogen Receptor (binary label: Positive/Negative) and it has been linked to the survival of patients: ER-positive patients are more likely to have a shorter survival than ER-negatives.\n",
    "\n",
    "`survival` is the patient's living status at the followup.\n",
    "\n",
    "We will use `ER` as our target variable for a machine learning model able to predict ER status from gene expression.\n",
    "\n",
    "For the remaining part of this hands-on, we need the data and labels to be stored in Numpy arrays (`.values` method): let's start with the labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6RAvCkTE3bns"
   },
   "outputs": [],
   "source": [
    "y_train = data_tr[\"ER\"].values.ravel()\n",
    "y_test = data_ts[\"ER\"].values.ravel()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `.ravel()` method returns a flattened 1-D array:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we drop the target columns from the data, convert to Numpy array, and store the result in a new variable:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6RAvCkTE3bns"
   },
   "outputs": [],
   "source": [
    "cols_to_remove = [\"ER\", \"survival\", \"stage\"]\n",
    "X_train = data_tr.drop(cols_to_remove, axis=1).values\n",
    "X_test = data_ts.drop(cols_to_remove, axis=1).values\n",
    "\n",
    "X_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PoeFheJ9p9wG"
   },
   "source": [
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "*Naming conventions: in the machine learning world, usually `x` is the data and `y` the target variable (the labels).*\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kqmSqRd23lCS"
   },
   "source": [
    "Always double-check the dimensions!\n",
    "\n",
    "When coding, it is a good practice to have a peek at the resulting variables, to be sure everything is OK: i.e., is that variable like it is supposed to be? Did I accidentally throw away a feature column?\n",
    "\n",
    "This can avoid lots of problems later on! Papers were even retracted because of this kind of errors..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XYs3b6JJ3qrn"
   },
   "outputs": [],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-VrsQZKO37wt"
   },
   "source": [
    "Let's go back to the sample labels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "iwI8uSvC4BbC"
   },
   "outputs": [],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_tr.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "f99gjxxW3yMs"
   },
   "source": [
    "---\n",
    "\n",
    "### Recap\n",
    "\n",
    "- **ER = Positive** indicates **worse** outcome (survival)\n",
    "- **ER = Negative** indicates **better** outcome\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Data preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The downstream analysis can benefit from data preprocessing, i.e., rescaling or standardizing data values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Scikit learn you can use `MinMaxScaler` or `StandardScaler` in the `preprocessing` submodule. Here is an example using `StandardScaler`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "## first you need to create a \"scaler\" object\n",
    "scaler = StandardScaler()\n",
    "## then you actually scale data by fitting the scaler object on the data...\n",
    "scaler.fit(X_train)\n",
    "## ... and using it to transform the data\n",
    "x_tr = scaler.transform(X_train)\n",
    "## note that we don't fit the scaler on the test set: we just transform it\n",
    "x_ts = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note how we transformed the test set: we computed the scaling parameters on the training set and applied them to the test set. In this way, we did not use any information in the test set to standardize it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The labels are in text form (\"Positive\", \"Negative\"). While this can be dealt with by the classifier, in general it is better to encode them into a numerical form.\n",
    "\n",
    "This can be done using scikit-learn's `LabelEncoder`, which has a similar `fit`/`transform` API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "le = LabelEncoder()\n",
    "y_tr = le.fit_transform(y_train)\n",
    "y_ts = le.transform(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check out as usual:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_ts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the encoded space, '1' corresponds to 'Positive': it seems intuitive, but that's a happy coincidence (always check)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gDUP1kW8ou4-"
   },
   "source": [
    "# 2. Unsupervised data exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember the scatterplot matrix we showed on the 4-feature Iris data?\n",
    "\n",
    "_How should we visualize feature relationships on this dataset with ~20K features?_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "yrQII8Zm4SQ-"
   },
   "source": [
    "It is probably better to first reduce the dimensionality of our data.\n",
    "\n",
    "Principal Component Analysis (PCA) is one example of data dimesionality reduction technique. It finds a sequence of linear combination of the variables (called the _principal components_) that explain the maximum variance and summarize the most information in the data and are mutually uncorrelated."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LrgsMWEHoyzd"
   },
   "source": [
    "Let's perform an **unsupervised learning** task on our data set \"as is\" by decomposing it in its Principal Components.\n",
    "\n",
    "In scikit-learn, we can use the module PCA:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pEQSdHYi5F4k"
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_P4v7UyA5I_Z"
   },
   "source": [
    "So far we have a PCA _object_ but no transformation yet.\n",
    "\n",
    "To actually transform the data, we'll have to _fit_ the PCA object on the training data, and then _transforming_ them in the Principal Component space:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca.fit(x_tr)\n",
    "z_tr = pca.transform(x_tr)\n",
    "# or:\n",
    "# z_tr = pca.fit_transform(x_tr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_tr.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z_tr.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GSURQjcg6ERt"
   },
   "source": [
    "Let's have a look at the _variance ratio_, i.e. the percentage of the variance explained by each component:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Cma7FaOd6F1M",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(pca.explained_variance_ratio_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4oM7HM9d6UL6"
   },
   "source": [
    "Is it always convenient to visualize the first two principal components in a scatterplot, in order to get a first assessment of the goodness of the decomposition.\n",
    "\n",
    "We will color the points in the plot according to our sample labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "o8Ktj-fK6WWo"
   },
   "outputs": [],
   "source": [
    "f = plt.figure()\n",
    "plt.scatter(z_tr[:, 0], z_tr[:, 1], c=y_tr, cmap=\"coolwarm\")\n",
    "plt.title(\"PCA of Train data\")\n",
    "plt.xlabel(\"PC1\")\n",
    "plt.ylabel(\"PC2\")\n",
    "plt.show()\n",
    "f.savefig(\"PCA_train.pdf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1GHx_kFZ6k5T"
   },
   "source": [
    "Now we apply the transformation to the test data, plot it, and save it as PDF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit a PCA on the train & use it to transform the test set\n",
    "pca.fit(x_tr)\n",
    "z_ts = pca.transform(x_ts)\n",
    "\n",
    "## plot\n",
    "f = plt.figure()\n",
    "plt.scatter(z_ts[:, 0], z_ts[:, 1], c=y_ts, cmap=\"coolwarm\")\n",
    "plt.title(\"PCA of Test data\")\n",
    "plt.xlabel(\"PC1\")\n",
    "plt.ylabel(\"PC2\")\n",
    "plt.show()\n",
    "f.savefig(\"PCA_test.pdf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 UMAP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now perform a UMAP transformation of the data, recalling what we did on the Iris dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import umap\n",
    "reducer = umap.UMAP(n_neighbors=5, random_state=91)\n",
    "embedding = reducer.fit_transform(x_tr)\n",
    "embedding.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert from Numpy array to Pandas dataframe for more convenient plotting with `seaborn`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_2D = pd.DataFrame(embedding, columns=['UMAP1', 'UMAP2'])\n",
    "df_2D['class'] = y_tr\n",
    "df_2D.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "sns.scatterplot(x=\"UMAP1\", y=\"UMAP2\", hue=\"class\", data=df_2D)\n",
    "\n",
    "# the Matplotlib way:\n",
    "# plt.scatter(embedding[:, 0], embedding[:, 1], c=[sns.color_palette()[x] for x in y_tr])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we have a UMAP object projecting the training set into a low-dimensional space, we can transform new data (i.e., the test set) based on the existing embedding. \n",
    "\n",
    "We use the `transform` method on the reducer object, and plot the resulting transformation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_ts = reducer.transform(x_ts)\n",
    "\n",
    "df_2D_ts = pd.DataFrame(embedding_ts, columns=['UMAP1', 'UMAP2'])\n",
    "df_2D_ts['class'] = y_ts\n",
    "\n",
    "sns.scatterplot(x=\"UMAP1\", y=\"UMAP2\", hue=\"class\", data=df_2D_ts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can independently project the test set too:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reducer = umap.UMAP(n_neighbors=5, random_state=91)\n",
    "reducer.fit(x_ts)\n",
    "embedding_ts = reducer.transform(x_ts)\n",
    "\n",
    "df_2D_ts = pd.DataFrame(embedding_ts, columns=['UMAP1', 'UMAP2'])\n",
    "df_2D_ts['class'] = y_ts\n",
    "\n",
    "sns.scatterplot(x=\"UMAP1\", y=\"UMAP2\", hue=\"class\", data=df_2D_ts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FErPMN8F6sb9"
   },
   "source": [
    "# 3. Supervised Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "T7xRxdjO6vTY"
   },
   "source": [
    "## 3.1 k-NN classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LnHfIehl62Js"
   },
   "source": [
    "Based on the PCA we built on our data, we decide to try some supervised learning on them.\n",
    "\n",
    "Scikit-learn provides you access to several models via a very convenient _fit_ and _predict_ interface.\n",
    "\n",
    "For example, let's fit a **k-NN** model on the whole training data and then use it to predict the labels of the test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6P24VUwQk3XG"
   },
   "outputs": [],
   "source": [
    "knn = neighbors.KNeighborsClassifier(n_neighbors=5)\n",
    "knn.fit(x_tr, y_tr)\n",
    "y_pred_knn = knn.predict(x_ts) # predict labels on test data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Crr_2DwX7f1Z"
   },
   "source": [
    "_In general, a classifier has **parameters** that need to be tuned. Default choices are not good in all situations._\n",
    "\n",
    "_For example, in k-NN the main parameter is the **number of neighbors** used in the nearest neighbors algorithm._\n",
    "\n",
    "_More on this later!_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "aBsDZNnXk3XP"
   },
   "source": [
    "To evaluate the predictions we need some kind of metrics. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "L88ilJJG-Cnf"
   },
   "source": [
    "### Recap: confusion matrix\n",
    "\n",
    "In this example, the first row is class 0, so the confusion matrix will look like:\n",
    "\n",
    "|      |  |  Predicted  |    |\n",
    "|------|-----------|----|----|\n",
    "|      |           | 0 | 1  |\n",
    "| True | 0        | TN | FP |\n",
    "|      | 1         | FN | TP |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JISD2EVQ9Q9Z"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "conf = confusion_matrix(y_ts, y_pred_knn)\n",
    "conf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Kh9MqHB5cC43"
   },
   "source": [
    "The total number of class 0 test samples (AN = All Negatives) should be equal to the sum of the first row of the confusion matrix, i.e., TN + FP:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pZVN8GKKdOhy"
   },
   "outputs": [],
   "source": [
    "np.sum(y_ts==0) # total number of \"class 0\" samples in the test set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SoBqoaTGcDVy"
   },
   "source": [
    "Similarly for class 1, i.e., AP = All Positives = TP + FN:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1PVj7JbxdVk0"
   },
   "outputs": [],
   "source": [
    "np.sum(y_ts==1) # total number of \"class 1\" samples in the test set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kva3wkz5ddap"
   },
   "source": [
    "Compute the Accuracy, remembering/using the formula: \n",
    "\n",
    "ACC = (TN + TP) / (TN + TP + FN + FP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "q0emRGAvfWi4"
   },
   "outputs": [],
   "source": [
    "tp = conf[1, 1]\n",
    "tn = conf[0, 0]\n",
    "fp = conf[0, 1]\n",
    "fn = conf[1, 0]\n",
    "\n",
    "acc = (tn + tp) / (tn + tp + fn + fp)\n",
    "print(acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "iqD0_lXneEJM"
   },
   "source": [
    "Now compute the Sensitivity:\n",
    "\n",
    "SENS = TP / (TP + FN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "a9JlR-LNe5ZI"
   },
   "outputs": [],
   "source": [
    "tp / (tp + fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "opG9Fe6EkI_b"
   },
   "source": [
    "Computing metrics by hand is good, but what about a quicker option?\n",
    "\n",
    "As seen in the lectures, Scikit Learn offers a handy broad range of functions to evaluate your classifier through its submodule `metrics`.\n",
    "\n",
    "Let's compute the accuracy using the scikit-learn built-in function `accuracy_score`, taking as input the predicted labels (`y_pred_knn`) and the true labels (`y_ts`):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3KeLJcCbkSo6"
   },
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "metrics.accuracy_score(y_ts, y_pred_knn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "r7owZk6BmTVX"
   },
   "source": [
    "What about Sensitivity? The built-in function is called `recall_score`, as Recall is an alternate name for Sensitivity:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MgfhssjZmsg3"
   },
   "outputs": [],
   "source": [
    "metrics.recall_score(y_ts, y_pred_knn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8ysD26nCnTSg"
   },
   "source": [
    "Scikit-learn also provides a neat `metrics.classification_report` function that outputs a few metrics stratified by class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HXgvIJM2k3XQ",
    "outputId": "0d2d0773-a292-40cb-d8e7-df6b6ee29ff2"
   },
   "outputs": [],
   "source": [
    "print(metrics.classification_report(y_ts, y_pred_knn))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mqAX0k6UoeZh"
   },
   "source": [
    "Let's consider the Matthews Correlation Coefficient (MCC):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Dl82PZ0Zn5lN"
   },
   "source": [
    "![MCC formula](https://www.researchgate.net/profile/Pablo_Moscato/publication/223966631/figure/fig1/AS:305103086080001@1449753652505/Calculation-of-Matthews-Correlation-Coefficient-MCC-A-Contingency-matrix_W640.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4v0aWAAJrex-"
   },
   "source": [
    "*Q: Do you remember the main features of MCC?*\n",
    "\n",
    "In scikit-learn it is computed by the `metrics.matthews_corrcoef` function.\n",
    "\n",
    "If we get the MCC for our kNN predictions, we can observe that it is in line with our *a priori* knowledge of the dataset (from the article):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OuoRfictk3XW",
    "outputId": "9119acba-9d18-4076-eb3c-8346ba420579"
   },
   "outputs": [],
   "source": [
    "print(metrics.matthews_corrcoef(y_ts, y_pred_knn))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HtPbufFSpSxL"
   },
   "source": [
    "*Compare the metrics that you computed so far. What can you say about this classification task? Does the classifier learn something?*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-d0e4tAbs7tP"
   },
   "source": [
    "The metrics may look good (e.g., accuracy around 0.9, MCC above 0.6) but...\n",
    "\n",
    "... how do you know if this model performs similarly well on unseen data?\n",
    "\n",
    "In other words, does this model *generalize* beyond its training set?\n",
    "\n",
    "This is why *data partitioning* techniques are used."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "13LEke6Js720"
   },
   "source": [
    "## Data partitioning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-IIhs_0Exo1B"
   },
   "source": [
    "### Hold-out strategy\n",
    "\n",
    "The idea behind data partitioning is to split your original data set into a **train** portion (for developing your machine learning model) and a **test** portion (for evaluating the performance of the trained model).\n",
    "\n",
    "The simplest and most straightforward way to partition your data set is to randomly split it in two groups (*hold-out strategy*).\n",
    "\n",
    "---\n",
    "\n",
    "\"But we already have a dataset split into train and test!\", you may object.\n",
    "\n",
    "That's perfectly fine! \n",
    "\n",
    "In fact, the train portion can be used to train a classifier in a cross-validation setting, as we will see. The test portion is then used only for inference.\n",
    "\n",
    "For the sake of this tutorial, we will further split the neuroblastoma train set into two subsets.\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "You achieve this using scikit-learn's function `train_test_split`, in the `model_selection` submodule.\n",
    "\n",
    "For example, let's split the data (`X_train`) into 80% train and 20% test (note the argument `test_size=0.2`), preserving class label proportions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tqbLHGVP2cAH"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "x_tr, x_ts, y_tr, y_ts = train_test_split(X_train, y_train, test_size=0.2, stratify=y_train, random_state=101)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* `stratify` is used to maintain class proportions in the splits;\n",
    "* `random_state` is the seed for the pseudo-random number generator (PRNG) - more on this below!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_tr.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TOKwQzhK8tVY"
   },
   "source": [
    "\n",
    "\n",
    "---\n",
    "\n",
    "What is the random_state?\n",
    "\n",
    "Whenever randomness is involved in a computer program, we need to rely on some sort of workaround because computers follow their instructions blindly and they are therefore completely predictable.\n",
    "\n",
    "One approach relies on *Pseudo-Random Number Generators* (PRNGs). \n",
    "\n",
    "PNRGs are algorithms that use mathematical formulas or precalculated tables to produce sequences of numbers that appear random. \n",
    "\n",
    "PNRGs are initialized by a *seed* (an integer), so that *the same seed yields the same sequence of pseudo-random numbers*. This is useful for reproduciblity.\n",
    "\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember to rescale the new training/test sets and to encode the labels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "x_tr = scaler.fit_transform(x_tr)\n",
    "x_ts = scaler.transform(x_ts)\n",
    "\n",
    "le = LabelEncoder()\n",
    "y_tr = le.fit_transform(y_tr)\n",
    "y_ts = le.transform(y_ts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "le9GlyN3xo9x"
   },
   "source": [
    "*Now, retrain a kNN model on X_train and evaluate its performance on X_test. Try using different random states for data splitting.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "n12boA3k3Neo"
   },
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "knn = neighbors.KNeighborsClassifier(n_neighbors=5)\n",
    "knn.fit(x_tr, y_tr)\n",
    "y_pred_knn = knn.predict(x_ts)\n",
    "\n",
    "acc = metrics.accuracy_score(y_ts, y_pred_knn)\n",
    "mcc = metrics.matthews_corrcoef(y_ts, y_pred_knn)\n",
    "\n",
    "print(f\"Accuracy = {acc:.3f}\")\n",
    "print(f\"MCC = {mcc:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# More ideas\n",
    "\n",
    "* Fit a model on the \"tr\" set, predict on \"ts\", evaluate the performance\n",
    "* Find a way to determine the \"optimal\" K for the kNN model\n",
    "* Use the other labels (\"survival\", \"stage\") to train a classifier\n",
    "* Evaluate the performance for each type of label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What next?\n",
    "\n",
    "* Cross-validation\n",
    "* Other classifiers: Support vector machines, Random Forests, Neural nets\n",
    "* Feature ranking\n",
    "* Parameter tuning"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "practical_neuroblastoma_partI_v0.2.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
